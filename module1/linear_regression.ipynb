{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Estimated time needed: **45** minutes\n",
    "\n",
    "If you are consulting an automobile company, you are trying to understand the factors that influence the sale price of the cars. Specifically, which factors drive the car prices up? And how accurately can you predict the sale price based on the car's features?\n",
    "\n",
    "In this notebook, we will perform a simple linear regression analysis on a car price dataset, show how this prediction analysis is done and what are the important assumptions that must be satisfied for linear regression. We will also look at different ways to transform our data.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "\n",
    "*   Select the significant features based on the visual analysis\n",
    "*   Check the assumptions for Linear Regression model\n",
    "*   Apply the Linear Regression model and make the predictions\n",
    "*   Apply the pipelines to transform the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Setup**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, we will be using the following libraries:\n",
    " - [`pandas`](https://pandas.pydata.org/) for managing the data.\n",
    " - [`numpy`](https://numpy.org/) for mathematical operations.\n",
    " - [`seaborn`](https://seaborn.pydata.org/) for visualizing the data.\n",
    " - [`matplotlib`](https://matplotlib.org/) for visualizing the data.\n",
    " - [`sklearn`](https://scikit-learn.org/stable/) for machine learning and machine-learning-pipeline related functions.\n",
    " - [`scipy`](https://docs.scipy.org/doc/scipy/tutorial/stats.html/) for statistical computations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import the required libraries**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following required modules are pre-installed in the Skills Network Labs environment. However if you run this notebook commands in a different Jupyter environment (e.g. Watson Studio or Ananconda) you will need to install these libraries by removing the `#` sign before `!mamba` in the code cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n",
    "#!pip install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==0.20.1\n",
    "# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.0-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.23.2 (from pandas)\n",
      "  Downloading numpy-2.3.1-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ravin\\miniconda3\\envs\\supervised_ml\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ravin\\miniconda3\\envs\\supervised_ml\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.0-cp311-cp311-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 2.1/11.1 MB 9.8 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 3.7/11.1 MB 9.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.2/11.1 MB 8.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 6.8/11.1 MB 8.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.4/11.1 MB 8.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.0/11.1 MB 8.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.1/11.1 MB 7.8 MB/s eta 0:00:00\n",
      "Downloading numpy-2.3.1-cp311-cp311-win_amd64.whl (13.0 MB)\n",
      "   ---------------------------------------- 0.0/13.0 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.6/13.0 MB 9.3 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 3.1/13.0 MB 8.0 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 4.7/13.0 MB 7.7 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.8/13.0 MB 7.2 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 7.6/13.0 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 9.2/13.0 MB 7.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.7/13.0 MB 7.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 12.3/13.0 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.0/13.0 MB 7.5 MB/s eta 0:00:00\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ---------------------------------------- 4/4 [pandas]\n",
      "\n",
      "Successfully installed numpy-2.3.1 pandas-2.3.0 pytz-2025.2 tzdata-2025.2\n",
      "Requirement already satisfied: numpy in c:\\users\\ravin\\miniconda3\\envs\\supervised_ml\\lib\\site-packages (2.3.1)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.3-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.2-cp311-cp311-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.58.4-cp311-cp311-win_amd64.whl.metadata (108 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp311-cp311-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\ravin\\miniconda3\\envs\\supervised_ml\\lib\\site-packages (from matplotlib) (2.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ravin\\miniconda3\\envs\\supervised_ml\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.2.1-cp311-cp311-win_amd64.whl.metadata (9.1 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ravin\\miniconda3\\envs\\supervised_ml\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ravin\\miniconda3\\envs\\supervised_ml\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.3-cp311-cp311-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 1.6/8.1 MB 7.6 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 2.9/8.1 MB 7.0 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 4.5/8.1 MB 7.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.6/8.1 MB 7.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.1/8.1 MB 7.9 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.3.2-cp311-cp311-win_amd64.whl (222 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.58.4-cp311-cp311-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ----------------------- ---------------- 1.3/2.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 7.5 MB/s eta 0:00:00\n",
      "Downloading kiwisolver-1.4.8-cp311-cp311-win_amd64.whl (71 kB)\n",
      "Downloading pillow-11.2.1-cp311-cp311-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 1.3/2.7 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 7.7 MB/s eta 0:00:00\n",
      "Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "\n",
      "   ----- ---------------------------------- 1/7 [pillow]\n",
      "   ----- ---------------------------------- 1/7 [pillow]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------------- 7/7 [matplotlib]\n",
      "\n",
      "Successfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.58.4 kiwisolver-1.4.8 matplotlib-3.10.3 pillow-11.2.1 pyparsing-3.2.3\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\ravin\\miniconda3\\envs\\supervised_ml\\lib\\site-packages (from seaborn) (2.3.1)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\ravin\\miniconda3\\envs\\supervised_ml\\lib\\site-packages (from seaborn) (2.3.0)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\ravin\\miniconda3\\envs\\supervised_ml\\lib\\site-packages (from seaborn) (3.10.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ravin\\miniconda3\\envs\\supervised_ml\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ravin\\miniconda3\\envs\\supervised_ml\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ravin\\miniconda3\\envs\\supervised_ml\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\ravin\\miniconda3\\envs\\supervised_ml\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ravin\\miniconda3\\envs\\supervised_ml\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\ravin\\miniconda3\\envs\\supervised_ml\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\ravin\\miniconda3\\envs\\supervised_ml\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ravin\\miniconda3\\envs\\supervised_ml\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ravin\\miniconda3\\envs\\supervised_ml\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ravin\\miniconda3\\envs\\supervised_ml\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ravin\\miniconda3\\envs\\supervised_ml\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\ravin\\miniconda3\\envs\\supervised_ml\\lib\\site-packages (from scikit-learn) (2.3.1)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.3-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.7.0-cp311-cp311-win_amd64.whl (10.7 MB)\n",
      "   ---------------------------------------- 0.0/10.7 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.8/10.7 MB 11.2 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 3.7/10.7 MB 9.1 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 5.2/10.7 MB 8.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 6.8/10.7 MB 8.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.9/10.7 MB 7.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.0/10.7 MB 8.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.7/10.7 MB 7.7 MB/s eta 0:00:00\n",
      "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading scipy-1.15.3-cp311-cp311-win_amd64.whl (41.2 MB)\n",
      "   ---------------------------------------- 0.0/41.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.8/41.2 MB 10.0 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 3.4/41.2 MB 9.1 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 5.0/41.2 MB 8.4 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 6.3/41.2 MB 7.7 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 7.6/41.2 MB 7.3 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 9.7/41.2 MB 7.8 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 11.0/41.2 MB 7.8 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 12.8/41.2 MB 7.7 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 14.2/41.2 MB 7.6 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 15.7/41.2 MB 7.7 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 17.3/41.2 MB 7.6 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 18.4/41.2 MB 7.5 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 19.1/41.2 MB 7.2 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 20.7/41.2 MB 7.1 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 21.8/41.2 MB 7.1 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 22.8/41.2 MB 6.9 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 24.1/41.2 MB 6.9 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 26.2/41.2 MB 7.0 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 27.5/41.2 MB 7.0 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 29.4/41.2 MB 7.1 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 30.7/41.2 MB 7.1 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 32.2/41.2 MB 7.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 33.8/41.2 MB 7.1 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 35.4/41.2 MB 7.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 37.0/41.2 MB 7.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 38.5/41.2 MB 7.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 40.1/41.2 MB 7.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  41.2/41.2 MB 7.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 41.2/41.2 MB 7.1 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ---------------------------------------- 4/4 [scikit-learn]\n",
      "\n",
      "Successfully installed joblib-1.5.1 scikit-learn-1.7.0 scipy-1.15.3 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-Learn 1.7.0\n"
     ]
    }
   ],
   "source": [
    "import sklearn; \n",
    "print(\"Scikit-Learn\", sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surpress warnings:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "import seaborn as sns \n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from scipy.stats import boxcox\n",
    "from scipy.stats.mstats import normaltest\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score \n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Reading and understanding our data**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, we will be using the car sales dataset, hosted on IBM Cloud object storage. This dataset can also be found and downloaded from [kaggle.com](https://www.kaggle.com/datasets/goyalshalini93/car-data), an open public data source.\n",
    "The dataset contains all the information about cars, a name of a manufacturer, all car's technical parameters and a sale price of a car.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the data into *pandas* data frame and look at the first 5 rows using the `head()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car_ID</th>\n",
       "      <th>symboling</th>\n",
       "      <th>CarName</th>\n",
       "      <th>fueltype</th>\n",
       "      <th>aspiration</th>\n",
       "      <th>doornumber</th>\n",
       "      <th>carbody</th>\n",
       "      <th>drivewheel</th>\n",
       "      <th>enginelocation</th>\n",
       "      <th>wheelbase</th>\n",
       "      <th>...</th>\n",
       "      <th>enginesize</th>\n",
       "      <th>fuelsystem</th>\n",
       "      <th>boreratio</th>\n",
       "      <th>stroke</th>\n",
       "      <th>compressionratio</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>peakrpm</th>\n",
       "      <th>citympg</th>\n",
       "      <th>highwaympg</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>alfa-romero giulia</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>two</td>\n",
       "      <td>convertible</td>\n",
       "      <td>rwd</td>\n",
       "      <td>front</td>\n",
       "      <td>88.6</td>\n",
       "      <td>...</td>\n",
       "      <td>130</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.47</td>\n",
       "      <td>2.68</td>\n",
       "      <td>9.0</td>\n",
       "      <td>111</td>\n",
       "      <td>5000</td>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "      <td>13495.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>alfa-romero stelvio</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>two</td>\n",
       "      <td>convertible</td>\n",
       "      <td>rwd</td>\n",
       "      <td>front</td>\n",
       "      <td>88.6</td>\n",
       "      <td>...</td>\n",
       "      <td>130</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.47</td>\n",
       "      <td>2.68</td>\n",
       "      <td>9.0</td>\n",
       "      <td>111</td>\n",
       "      <td>5000</td>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "      <td>16500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>alfa-romero Quadrifoglio</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>two</td>\n",
       "      <td>hatchback</td>\n",
       "      <td>rwd</td>\n",
       "      <td>front</td>\n",
       "      <td>94.5</td>\n",
       "      <td>...</td>\n",
       "      <td>152</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>2.68</td>\n",
       "      <td>3.47</td>\n",
       "      <td>9.0</td>\n",
       "      <td>154</td>\n",
       "      <td>5000</td>\n",
       "      <td>19</td>\n",
       "      <td>26</td>\n",
       "      <td>16500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>audi 100 ls</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>four</td>\n",
       "      <td>sedan</td>\n",
       "      <td>fwd</td>\n",
       "      <td>front</td>\n",
       "      <td>99.8</td>\n",
       "      <td>...</td>\n",
       "      <td>109</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.40</td>\n",
       "      <td>10.0</td>\n",
       "      <td>102</td>\n",
       "      <td>5500</td>\n",
       "      <td>24</td>\n",
       "      <td>30</td>\n",
       "      <td>13950.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>audi 100ls</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>four</td>\n",
       "      <td>sedan</td>\n",
       "      <td>4wd</td>\n",
       "      <td>front</td>\n",
       "      <td>99.4</td>\n",
       "      <td>...</td>\n",
       "      <td>136</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.40</td>\n",
       "      <td>8.0</td>\n",
       "      <td>115</td>\n",
       "      <td>5500</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>17450.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   car_ID  symboling                   CarName fueltype aspiration doornumber  \\\n",
       "0       1          3        alfa-romero giulia      gas        std        two   \n",
       "1       2          3       alfa-romero stelvio      gas        std        two   \n",
       "2       3          1  alfa-romero Quadrifoglio      gas        std        two   \n",
       "3       4          2               audi 100 ls      gas        std       four   \n",
       "4       5          2                audi 100ls      gas        std       four   \n",
       "\n",
       "       carbody drivewheel enginelocation  wheelbase  ...  enginesize  \\\n",
       "0  convertible        rwd          front       88.6  ...         130   \n",
       "1  convertible        rwd          front       88.6  ...         130   \n",
       "2    hatchback        rwd          front       94.5  ...         152   \n",
       "3        sedan        fwd          front       99.8  ...         109   \n",
       "4        sedan        4wd          front       99.4  ...         136   \n",
       "\n",
       "   fuelsystem  boreratio  stroke compressionratio horsepower  peakrpm citympg  \\\n",
       "0        mpfi       3.47    2.68              9.0        111     5000      21   \n",
       "1        mpfi       3.47    2.68              9.0        111     5000      21   \n",
       "2        mpfi       2.68    3.47              9.0        154     5000      19   \n",
       "3        mpfi       3.19    3.40             10.0        102     5500      24   \n",
       "4        mpfi       3.19    3.40              8.0        115     5500      18   \n",
       "\n",
       "   highwaympg    price  \n",
       "0          27  13495.0  \n",
       "1          27  16500.0  \n",
       "2          26  16500.0  \n",
       "3          30  13950.0  \n",
       "4          22  17450.0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML240EN-SkillsNetwork/labs/data/CarPrice_Assignment.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find more information about the features and types using the `info()`  method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 205 entries, 0 to 204\n",
      "Data columns (total 26 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   car_ID            205 non-null    int64  \n",
      " 1   symboling         205 non-null    int64  \n",
      " 2   CarName           205 non-null    object \n",
      " 3   fueltype          205 non-null    object \n",
      " 4   aspiration        205 non-null    object \n",
      " 5   doornumber        205 non-null    object \n",
      " 6   carbody           205 non-null    object \n",
      " 7   drivewheel        205 non-null    object \n",
      " 8   enginelocation    205 non-null    object \n",
      " 9   wheelbase         205 non-null    float64\n",
      " 10  carlength         205 non-null    float64\n",
      " 11  carwidth          205 non-null    float64\n",
      " 12  carheight         205 non-null    float64\n",
      " 13  curbweight        205 non-null    int64  \n",
      " 14  enginetype        205 non-null    object \n",
      " 15  cylindernumber    205 non-null    object \n",
      " 16  enginesize        205 non-null    int64  \n",
      " 17  fuelsystem        205 non-null    object \n",
      " 18  boreratio         205 non-null    float64\n",
      " 19  stroke            205 non-null    float64\n",
      " 20  compressionratio  205 non-null    float64\n",
      " 21  horsepower        205 non-null    int64  \n",
      " 22  peakrpm           205 non-null    int64  \n",
      " 23  citympg           205 non-null    int64  \n",
      " 24  highwaympg        205 non-null    int64  \n",
      " 25  price             205 non-null    float64\n",
      "dtypes: float64(8), int64(8), object(10)\n",
      "memory usage: 41.8+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the output above, we have 205 entries or rows, as well as 26 features. The \"Non-Null Count\" column shows the number of non-null entries.  If the count is 205 then there is no missing values for that particular feature. The 'price' is our target, or response variable, and the rest of the features are our predictor variables.\n",
    "\n",
    "We also have a mix of numerical (8 int64 and 8 float64) and object data types (10 object). \n",
    "\n",
    "The `describe()` function will provide the statistical information about all numeric values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car_ID</th>\n",
       "      <th>symboling</th>\n",
       "      <th>wheelbase</th>\n",
       "      <th>carlength</th>\n",
       "      <th>carwidth</th>\n",
       "      <th>carheight</th>\n",
       "      <th>curbweight</th>\n",
       "      <th>enginesize</th>\n",
       "      <th>boreratio</th>\n",
       "      <th>stroke</th>\n",
       "      <th>compressionratio</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>peakrpm</th>\n",
       "      <th>citympg</th>\n",
       "      <th>highwaympg</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>205.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>205.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>103.000000</td>\n",
       "      <td>0.834146</td>\n",
       "      <td>98.756585</td>\n",
       "      <td>174.049268</td>\n",
       "      <td>65.907805</td>\n",
       "      <td>53.724878</td>\n",
       "      <td>2555.565854</td>\n",
       "      <td>126.907317</td>\n",
       "      <td>3.329756</td>\n",
       "      <td>3.255415</td>\n",
       "      <td>10.142537</td>\n",
       "      <td>104.117073</td>\n",
       "      <td>5125.121951</td>\n",
       "      <td>25.219512</td>\n",
       "      <td>30.751220</td>\n",
       "      <td>13276.710571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>59.322565</td>\n",
       "      <td>1.245307</td>\n",
       "      <td>6.021776</td>\n",
       "      <td>12.337289</td>\n",
       "      <td>2.145204</td>\n",
       "      <td>2.443522</td>\n",
       "      <td>520.680204</td>\n",
       "      <td>41.642693</td>\n",
       "      <td>0.270844</td>\n",
       "      <td>0.313597</td>\n",
       "      <td>3.972040</td>\n",
       "      <td>39.544167</td>\n",
       "      <td>476.985643</td>\n",
       "      <td>6.542142</td>\n",
       "      <td>6.886443</td>\n",
       "      <td>7988.852332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>86.600000</td>\n",
       "      <td>141.100000</td>\n",
       "      <td>60.300000</td>\n",
       "      <td>47.800000</td>\n",
       "      <td>1488.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>2.540000</td>\n",
       "      <td>2.070000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>4150.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>5118.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>52.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>94.500000</td>\n",
       "      <td>166.300000</td>\n",
       "      <td>64.100000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>2145.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>3.150000</td>\n",
       "      <td>3.110000</td>\n",
       "      <td>8.600000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>7788.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>103.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>173.200000</td>\n",
       "      <td>65.500000</td>\n",
       "      <td>54.100000</td>\n",
       "      <td>2414.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>3.310000</td>\n",
       "      <td>3.290000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>5200.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>10295.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>154.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>102.400000</td>\n",
       "      <td>183.100000</td>\n",
       "      <td>66.900000</td>\n",
       "      <td>55.500000</td>\n",
       "      <td>2935.000000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>3.580000</td>\n",
       "      <td>3.410000</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>5500.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>16503.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>205.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>120.900000</td>\n",
       "      <td>208.100000</td>\n",
       "      <td>72.300000</td>\n",
       "      <td>59.800000</td>\n",
       "      <td>4066.000000</td>\n",
       "      <td>326.000000</td>\n",
       "      <td>3.940000</td>\n",
       "      <td>4.170000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>288.000000</td>\n",
       "      <td>6600.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>45400.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           car_ID   symboling   wheelbase   carlength    carwidth   carheight  \\\n",
       "count  205.000000  205.000000  205.000000  205.000000  205.000000  205.000000   \n",
       "mean   103.000000    0.834146   98.756585  174.049268   65.907805   53.724878   \n",
       "std     59.322565    1.245307    6.021776   12.337289    2.145204    2.443522   \n",
       "min      1.000000   -2.000000   86.600000  141.100000   60.300000   47.800000   \n",
       "25%     52.000000    0.000000   94.500000  166.300000   64.100000   52.000000   \n",
       "50%    103.000000    1.000000   97.000000  173.200000   65.500000   54.100000   \n",
       "75%    154.000000    2.000000  102.400000  183.100000   66.900000   55.500000   \n",
       "max    205.000000    3.000000  120.900000  208.100000   72.300000   59.800000   \n",
       "\n",
       "        curbweight  enginesize   boreratio      stroke  compressionratio  \\\n",
       "count   205.000000  205.000000  205.000000  205.000000        205.000000   \n",
       "mean   2555.565854  126.907317    3.329756    3.255415         10.142537   \n",
       "std     520.680204   41.642693    0.270844    0.313597          3.972040   \n",
       "min    1488.000000   61.000000    2.540000    2.070000          7.000000   \n",
       "25%    2145.000000   97.000000    3.150000    3.110000          8.600000   \n",
       "50%    2414.000000  120.000000    3.310000    3.290000          9.000000   \n",
       "75%    2935.000000  141.000000    3.580000    3.410000          9.400000   \n",
       "max    4066.000000  326.000000    3.940000    4.170000         23.000000   \n",
       "\n",
       "       horsepower      peakrpm     citympg  highwaympg         price  \n",
       "count  205.000000   205.000000  205.000000  205.000000    205.000000  \n",
       "mean   104.117073  5125.121951   25.219512   30.751220  13276.710571  \n",
       "std     39.544167   476.985643    6.542142    6.886443   7988.852332  \n",
       "min     48.000000  4150.000000   13.000000   16.000000   5118.000000  \n",
       "25%     70.000000  4800.000000   19.000000   25.000000   7788.000000  \n",
       "50%     95.000000  5200.000000   24.000000   30.000000  10295.000000  \n",
       "75%    116.000000  5500.000000   30.000000   34.000000  16503.000000  \n",
       "max    288.000000  6600.000000   49.000000   54.000000  45400.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Cleaning and Wrangling**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will check if we have any missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "car_ID              0\n",
       "symboling           0\n",
       "CarName             0\n",
       "fueltype            0\n",
       "aspiration          0\n",
       "doornumber          0\n",
       "carbody             0\n",
       "drivewheel          0\n",
       "enginelocation      0\n",
       "wheelbase           0\n",
       "carlength           0\n",
       "carwidth            0\n",
       "carheight           0\n",
       "curbweight          0\n",
       "enginetype          0\n",
       "cylindernumber      0\n",
       "enginesize          0\n",
       "fuelsystem          0\n",
       "boreratio           0\n",
       "stroke              0\n",
       "compressionratio    0\n",
       "horsepower          0\n",
       "peakrpm             0\n",
       "citympg             0\n",
       "highwaympg          0\n",
       "price               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, check for any duplicates by running `duplicated()` function through 'car_ID' records, since each row has a unique car ID value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(data.duplicated(subset = 'car_ID')) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's look into some of our object variables first. Using `unique()` function, we will describe all categories of the 'CarName' attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"CarName\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the 'CarName' includes both the company name (brand) and the car model. Next, we want to split a company name from the model of a car, as for our model building purpose, we will focus on a company name only. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['brand'] = data.CarName.str.split(' ').str.get(0).str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view all the `unique()` brands now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.brand.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some typos in the names of the cars, so they should be corrected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['brand'] = data['brand'].replace(['vw', 'vokswagen'], 'volkswagen')\n",
    "data['brand'] = data['brand'].replace(['maxda'], 'mazda')\n",
    "data['brand'] = data['brand'].replace(['porcshce'], 'porsche')\n",
    "data['brand'] = data['brand'].replace(['toyouta'], 'toyota')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.brand.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot and sort the total number of Brands.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15,5))\n",
    "plt1 = sns.countplot(data['brand'], order=pd.value_counts(data['brand']).index,)\n",
    "plt1.set(xlabel = 'Brand', ylabel= 'Count of Cars')\n",
    "plt.show()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can drop 'car_ID', 'symboling', and 'CarName' from our data frame, since they will no longer be needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['car_ID', 'symboling', 'CarName'],axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you need to save this partially processed data, uncomment the line below.\n",
    "#data.to_csv('cleaned_car_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "In this exercise, explore any (or all) object variables of your interest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code and run the cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution</strong> (Click Here)</summary>\n",
    "```python\n",
    "data.fueltype.unique()\n",
    "data[\"enginelocation\"].value_counts()\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to engineer some features, for better visualizations and analysis. We will group our data by 'brand', calculate the average price for each brand, and split these prices into 3 bins: 'Budget', 'Mid-Range', and 'Luxury' cars, naming the newly created column - the 'brand_category'. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_comp_avg_price = data[['brand','price']].groupby('brand', as_index = False).mean().rename(columns={'price':'brand_avg_price'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.merge(data_comp_avg_price, on = 'brand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now check the statistics of our average car price per car brand.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.brand_avg_price.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['brand_category'] = data['brand_avg_price'].apply(lambda x : \"Budget\" if x < 10000 \n",
    "                                                     else (\"Mid_Range\" if 10000 <= x < 20000\n",
    "                                                           else \"Luxury\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exploratory Data Analysis**\n",
    "\n",
    "List of Categorical Variables:\n",
    "- brand_category\n",
    "- fueltype\n",
    "- enginetype\n",
    "- carbody\n",
    "- doornumber\n",
    "- enginelocation\n",
    "- fuelsystem\n",
    "- cylindernumber\n",
    "- aspiration\n",
    "- drivewheel\n",
    "\n",
    "We will use the `boxplot()` function on the above mentioned categorical variables, to display the mean, variance, and possible outliers, with respect to the price.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 20))\n",
    "plt.subplot(4,2,1)\n",
    "sns.boxplot(x = 'fueltype', y = 'price', data = data)\n",
    "plt.subplot(4,2,2)\n",
    "sns.boxplot(x = 'aspiration', y = 'price', data = data)\n",
    "plt.subplot(4,2,3)\n",
    "sns.boxplot(x = 'carbody', y = 'price', data = data)\n",
    "plt.subplot(4,2,4)\n",
    "sns.boxplot(x = 'drivewheel', y = 'price', data = data)\n",
    "plt.subplot(4,2,5)\n",
    "sns.boxplot(x = 'enginetype', y = 'price', data = data)\n",
    "plt.subplot(4,2,6)\n",
    "sns.boxplot(x = 'brand_category', y = 'price', data = data)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's view the list of top features that have high correlation coefficient. The `corr()` function calculates the Pearson'r correlation coefficients with respect to the 'price'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = data.corr(numeric_only=True)\n",
    "\n",
    "corr_matrix['price'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are strongly correlated numerical features with Car Price.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `heatmap()` or `pairplot()` to further explore the relationship between all features and the target variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 \n",
    "Use the `pairplot()` function to display the scatter plots of the relationships between the features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code and run the cell\n",
    "sns.pairplot(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution</strong> (Click Here)</summary>\n",
    "```python\n",
    "\n",
    "\n",
    "sns.pairplot(data)\n",
    "plt.show()   \n",
    "    \n",
    "    \n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Testing Assumptions for Linear Regression**\n",
    "Since we fit a linear model, we assume that the relationship between the target (price) and other features is linear. \n",
    "\n",
    "We also expect that the errors, or residuals, are pure random fluctuations around the true line, in other words, the variability in the response (dependent) variable doesn't increase as the value of the predictor (independent) variable increases. This is the assumption of equal variance, also known as *Homoscedasticity*. \n",
    "\n",
    "We also assume that the observations are independent of one another (no *multicollinearity*), and there is no correlation between the sequential observations.\n",
    "\n",
    "If we see one of these assumptions in the dataset are not met, it's more likely that the other ones, mentioned above, will also be violated. Luckily, we can check and fix these assumptions with a few unique techniques.\n",
    "\n",
    "Now, let's briefly touch upon each of these assumptions in our example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Linearity Assumption\n",
    "\n",
    "Linear regression needs the relationship between independent variable and the dependent variable to be linear. We can test this assumption with some scatter plots and regression lines. \n",
    "\n",
    "We will start with the 'enginesize' and 'horsepower' features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(figsize = (12,8), ncols=2,sharey=False)\n",
    "sns.scatterplot( x = data.enginesize, y = data.price,  ax=ax1)\n",
    "sns.regplot(x=data.enginesize, y=data.price, ax=ax1)\n",
    " \n",
    "sns.scatterplot(x = data.horsepower,y = data.price, ax=ax2)\n",
    "sns.regplot(x=data.horsepower, y=data.price, ax=ax2);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "In this Exercise, plot any other numeric features, using the *seaborn* `regplot()` function, to see whether there is any linear relationship between the feature and the 'price'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code and run the cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution</strong> (Click Here)</summary>\n",
    "```python\n",
    "sns.regplot(x=data.curbweight, y=data.price, data=data)\n",
    "    \n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. *Homoscedasticity*\n",
    "\n",
    "The assumption of *homoscedasticity* (constant variance), is crucial to linear regression models. *Homoscedasticity* describes a situation in which the error term or variance or the \"noise\" or random disturbance in the relationship between the independent variables and the dependent variable is the same across all values of the independent variable. In other words, there is a constant variance present in the response variable as the predictor variable increases. If the \"noise\" is not the same across the values of an independent variable, we call it *heteroscedasticity*, opposite of *homoscedasticity*. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize = (12,8))\n",
    "sns.residplot(data.enginesize, data.price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot, we can tell the error variance across the true line is dispersed somewhat not uniformly, but in a funnel like shape. So, the assumption of the *homoscedasticity* is more likely not met.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Normality\n",
    "The linear regression analysis requires the dependent variable, 'price', to be normally distributed. A histogram, box plot, or a Q-Q-Plot can check if the target variable is normally distributed. The goodness of fit test, e.g., the Kolmogorov-Smirnov test can check for normality in the dependent variable. [This documentation](https://towardsdatascience.com/normality-tests-in-python-31e04aa4f411) contains more information on the normality assumption. \n",
    "\n",
    "Let's display all three charts to show how our target variable, 'price' behaves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_3_chart(data, feature):\n",
    "    ## Importing seaborn, matplotlab and scipy modules. \n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.gridspec as gridspec\n",
    "    from scipy import stats\n",
    "    import matplotlib.style as style\n",
    "    style.use('fivethirtyeight')\n",
    "\n",
    "    ## Creating a customized chart. and giving in figsize and everything. \n",
    "    fig = plt.figure(constrained_layout=True, figsize=(12,8))\n",
    "    ## creating a grid of 3 cols and 3 rows. \n",
    "    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n",
    "    #gs = fig3.add_gridspec(3, 3)\n",
    "\n",
    "    ## Customizing the histogram grid. \n",
    "    ax1 = fig.add_subplot(grid[0, :2])\n",
    "    ## Set the title. \n",
    "    ax1.set_title('Histogram')\n",
    "    ## plot the histogram. \n",
    "    sns.distplot(data.loc[:,feature], norm_hist=True, ax = ax1)\n",
    "\n",
    "    # customizing the QQ_plot. \n",
    "    ax2 = fig.add_subplot(grid[1, :2])\n",
    "    ## Set the title. \n",
    "    ax2.set_title('QQ_plot')\n",
    "    ## Plotting the QQ_Plot. \n",
    "    stats.probplot(data.loc[:,feature], plot = ax2)\n",
    "\n",
    "    ## Customizing the Box Plot. \n",
    "    ax3 = fig.add_subplot(grid[:, 2])\n",
    "    ## Set title. \n",
    "    ax3.set_title('Box Plot')\n",
    "    ## Plotting the box plot. \n",
    "    sns.boxplot(data.loc[:,feature], orient='v', ax = ax3);\n",
    "    \n",
    "plotting_3_chart(data, 'price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These three charts above can tell us a lot about our target variable:\n",
    "\n",
    "- Our target variable, 'price' is not normally distributed\n",
    "- Our target variable is right-skewed\n",
    "- There are some outliers in the variable\n",
    "\n",
    "The right-skewed plot means that most prices in the dataset are on the lower end (below 15,000). The 'max' value is very far from the '75%' quantile statistic. All these plots show that the assumption for accurate linear regression modeling is not met. \n",
    "\n",
    "Next, we will perform the log transformation to correct our target variable and to make it more normally distributed. \n",
    "\n",
    "But first, we will save our data that we have changed so far, in the 'previous_data' frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_data = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Transformation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check statistically if the target is normally distributed, using `normaltest()` function. If the p-value is large (>0.05), the target variable is normally distributed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normaltest(data.price.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the p-value is very small, so it is not normally distributed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can try to transform our data, so it looks more normally distributed. We can use the `np.log()` or `np.log1p`functions from the `numpy` library to perform the log transformation. The `np.log1p` works better with smaller numbers and thus provides more accurate results. This [documentation](https://numpy.org/doc/stable/reference/generated/numpy.log.html) contains more information about the numpy log transform.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['price'] = np.log(data['price'])\n",
    "plotting_3_chart(data, 'price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our p-value, after the transformation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normaltest(data.price.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the log method transformed the car 'price' distribution into a more symmetrical bell curve. It is still not perfect, but it is much closer to being normally distributed.\n",
    "\n",
    "There are other ways to correct the skewed data. For example, Square Root Transform (`np.sqrt`) and the Box-Cox Transform (`stats.boxcox` from the `scipy stats` library). To learn more about these two methods, please check out this [article](https://towardsdatascience.com/top-3-methods-for-handling-skewed-data-1334e0debf45).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 \n",
    "Use the `boxcox()` function to do another transformation on the original, untransformed data (previous_data). Use the `normaltest()` function to check for statistics. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code and run the cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution</strong> (Click Here)</summary>\n",
    "```python\n",
    "cp_result = boxcox(previous_data.price)\n",
    "boxcox_price = cp_result[0]\n",
    "\n",
    "    \n",
    "normaltest(boxcox_price)\n",
    "    \n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Answer</strong> (Click Here)</summary>\n",
    "```python\n",
    "The higher the p-value is, the closer the distribution is to normal. In our case, pvalue=0.0.00023321005129893173, is very small, (<0.05), so the target variable is still not normally distributed).\n",
    "    \n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. *Multicollinearity*\n",
    "\n",
    "*Multicollinearity* is when there is a strong correlation between the independent variables. Linear regression or multilinear regression requires independent variables to have little or no similar features. *Multicollinearity* can lead to a variety of problems, including:\n",
    "- The effect of predictor variables estimated by our regression will depend on what other variables are included in our model.\n",
    "- Predictors can have widely different results depending on the observations in our sample, and small changes in samples can   result in very different estimated effects.\n",
    "- With very high multicollinearity, the inverse matrix, the computer calculations may not be accurate.\n",
    "- We can no longer interpret a coefficient on a variable because there is no scenario in which one variable can change without a conditional change in another variable.\n",
    "\n",
    "Using `heatmap()` function is an excellent way to identify whether there is *multicollinearity* present or not. The best way to solve for *multicollinearity* is to use the regularization methods like *Ridge* or *Lasso*, which we will introduce in the **Regularization** lab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "Use the `heatmap()` do display all correlation factors of the numeric variables. Do you see any correlations between the independent features?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code and run the cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution</strong> (Click Here)</summary>\n",
    "```python\n",
    "\n",
    "num = data.select_dtypes(include = ['int64', 'float64'])\n",
    "plt.figure(figsize = (30, 25))\n",
    "sns.heatmap(num.corr(), annot = True, cmap=\"YlGnBu\")\n",
    "plt.show()   \n",
    "    \n",
    "    \n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Answer</strong> (Click Here)</summary>\n",
    "```python\n",
    "\n",
    "Observation.\n",
    "As we can see, the multicollinearity still exists in various features. However, we will keep them for now for the sake of learning and let the models (e.x. Regularization models such as Lasso, Ridge in the next lab) do the clean up later on.\n",
    "    \n",
    "    \n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Linear Regression Model**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of significant variables after Exploratory Data Analysis :\n",
    "\n",
    "Numerical:\n",
    "\n",
    "- Curbweight \n",
    "- Car Length\n",
    "- Car width\n",
    "- Engine Size \n",
    "- Boreratio \n",
    "- Horse Power \n",
    "- Wheel base \n",
    "- City mpg (miles per gallon)\n",
    "- Highway mpg (miles per gallon)\n",
    "\n",
    "Categorical:\n",
    "\n",
    "- Engine Type \n",
    "- Fuel type \n",
    "- Car Body \n",
    "- Aspiration \n",
    "- Cylinder Number \n",
    "- Drivewheel \n",
    "- Brand Category\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to put all the selected features into a data frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['price', 'fueltype', 'aspiration','carbody', 'drivewheel','wheelbase', 'brand_category',\n",
    "                  'curbweight', 'enginetype', 'cylindernumber', 'enginesize', 'boreratio','horsepower', 'carlength','carwidth','citympg','highwaympg']\n",
    "\n",
    "\n",
    "\n",
    "selected = data[columns]\n",
    "selected.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the categorical columns by  iterating  through the ```dtypes```  attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns=[key for key, value in selected.dtypes.iteritems()  if value=='O']\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6 \n",
    "Find the names of the  numeric columns using the list ```columns``` and assign them to the list  ```numeric_columns```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code and run the cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution</strong> (Click Here)</summary>\n",
    "```python\n",
    "\n",
    "numeric_columns=list(set(columns)-set(categorical_columns))\n",
    "numeric_columns\n",
    "    \n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can split the data into the features ```X``` and target ```y```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = selected.drop(\"price\", axis=1)\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = selected[\"price\"].copy()\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we used one-hot encoding to deal with the categorical data, let's examine the distribution of the categorical variables:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in  categorical_columns:\n",
    "    print(\"column name:\", column)\n",
    "    print(\"value_count:\")\n",
    "    print( X[column].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see many categorical features have few or one occurrence. For example, we see ```three```, ```twelve``` only occur once in the column ```cylindernumber```. Therefore, if the components for the one-hot encoding are constructed using the training data, and the sample in the column ```cylindernumber``` does not include three or twelve, we will get an error. As a result, we must split the data before the transformation.   This is fine as one-hot encoding is a deterministic transform, but for other transforms, for example standardization, the parameters should be estimated using the training data, then applied to the test data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following modules:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform one-hot encoding, we use the ```ColumnTransformer``` class, this allows different columns or column subsets to be transformed separately. \n",
    "\n",
    "The input is as follows:\n",
    "\n",
    "The `transformerslist` is the number of tuples.\n",
    "The list of `(name, transformer, columns)` tuples specify the transformer objects to be applied to the subsets of the data.\n",
    "\n",
    "*   name: name of the operation that can be used later \n",
    "*  `transformer`: estimator must support fit and transform, in this case we will use `OneHotEncoder()`\n",
    "*  `‘drop’`: to  drop the columns \n",
    "*  `‘passthrough’`: to pass them through untransformed data\n",
    "*  `remainder`: specifies the columns that are not transformed are being set to `passthrough`. They are  combined in the output, and the non-specified columns are dropped.\n",
    "\n",
    "\n",
    "We apply ```fit_transform()``` to transform the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = ColumnTransformer(transformers=[(\"one_hot\", OneHotEncoder(), categorical_columns) ],remainder=\"passthrough\")\n",
    "X=one_hot.fit_transform(X)\n",
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the output is a NumPy array, so let's get the feature names from the ```one_hot``` object using  ```get_feature_names_out()``` method. The output  will be the feature name with the  prefix of the name of the transformer. For one-hot encoding, the prefix will also include the name of the column that generated that feature. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=one_hot.get_feature_names_out()\n",
    "names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's strip out the prefix of the string. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunm_names=[name[name.find(\"_\")+1:] for name in  [name[name.find(\"__\")+2:] for name in names]]\n",
    "colunm_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the result as a dataframe to be used in other labs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(data=X,columns=colunm_names)\n",
    "#df.to_csv('cleaned_car_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7\n",
    "Write the lines of code  that performs same task as  ``ColumnTransformer`` using ``OneHotEncoder()``.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code and run the cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<details>\n",
    "<summary><strong>Solution</strong> (Click Here)</summary>\n",
    "```python\n",
    "\n",
    "X_ = selected[categorical_columns+numeric_columns]\n",
    "\n",
    "\n",
    "X_numeric=X_[numeric_columns].to_numpy()\n",
    "X_categorical=OneHotEncoder().fit_transform(X_[categorical_columns]).toarray()\n",
    "X_=np.concatenate((X_categorical,X_numeric), axis = 1)\n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8\n",
    "Write the lines of code that performs same task as  ``ColumnTransformer`` using ``pd.get_dummies``.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code and run the cell\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution</strong> (Click Here)</summary>\n",
    "```python\n",
    "\n",
    "# Defining the map function\n",
    "def dummies(x,data):\n",
    "    temp = pd.get_dummies(data[x], drop_first = True)\n",
    "    data = pd.concat([data, temp], axis = 1)\n",
    "    data.drop([x], axis = 1, inplace = True)\n",
    "    return data\n",
    "\n",
    "X_ = selected[categorical_columns+numeric_columns]\n",
    "N_column=0\n",
    "for column in  categorical_columns:\n",
    "    print(pd.unique(data[column]))\n",
    "    X_ = dummies(column,X_)\n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following Module 2: Data Splits and Cross Validation section of the Course, we will learn more about train and test split of the data deeper. But for now, we use `train_test_split()` function from *sklearn.model_selection* library to split our data into training and testing sets, using 30% of the data for testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( df, y, test_size=0.30, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize the Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We standardize features by removing the mean and scaling to unit variance using ```StandardScaler```, we create a \n",
    "```StandardScaler``` object:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss=StandardScaler()\n",
    "ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We `fit` our training data, then we `transform` it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=ss.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we apply the `LinearRegression()` model and `fit()` our ```X``` and ```y``` data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression()\n",
    "lm.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will select some random data and apply the `predict()` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=ss.transform(X_test)\n",
    "car_price_predictions = lm.predict(X_test)\n",
    "car_price_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate this model with some statistics. We will use *Scikit_Learn's* `mean_squared_error()` function for this evaluation. MSE measures the average of the squares of the errors, that is, the average squared difference between the estimated values and the actual values using the test data. For more information on MSE, please visit this wikipedia [site](https://en.wikipedia.org/wiki/Mean_squared_error).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, car_price_predictions)\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the R squared, the coefficient of determination, which is the proportion of the variation in the dependent variable that is predictable from the independent variables. The closer is R squared to 1, the better is the fit of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `score()` method returns the coefficient of determination of the prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `r2_score` method returns the same statistic, also known as the goodness of fit of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_test,car_price_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the R squared is negative, it suggests the overfitting, when a statistical model fits exactly against its training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Object\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create a Pipeline object and apply a set of transforms sequentially. Then, we can apply linear regression.  Data Pipelines simplify the steps of processing the data. We use the module Pipeline to create a pipeline. We also use ```StandardScaler```as a step in our pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We create the pipeline, by creating a list of tuples including the name of the model or estimator and its corresponding constructor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps=[('scaler', StandardScaler()), ('lm',  LinearRegression())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We input the list as an argument to the pipeline constructor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We ```fit``` the constructor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make a prediction and perform model evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_price_predictions = pipe.predict(X_test)\n",
    "mse = mean_squared_error(y_test, car_price_predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "rmse\n",
    "r2_score(car_price_predictions, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9 \n",
    "Use the  ``ColumnTransformer`` in the pipeline, then train the model using <b>all</b> the data, make a prediction and calculate all the  metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code and run the cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution</strong> (Click Here)</summary>\n",
    "```python\n",
    "\n",
    "X = selected[categorical_columns+numeric_columns]\n",
    "one_hot = ColumnTransformer(transformers=[(\"one_hot\", OneHotEncoder(), categorical_columns) ],remainder=\"passthrough\")\n",
    "steps=[('one_hot',one_hot), ('scaler', StandardScaler()), ('lm',  LinearRegression())]\n",
    "\n",
    "pipe = Pipeline(steps=steps)\n",
    "pipe.fit(X,y)\n",
    "car_price_predictions=pipe.predict(X)\n",
    "r2_score(car_price_predictions, y)\n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations! - You have completed the lab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Svitlana Kramar](www.linkedin.com/in/svitlana-kramar)\n",
    "\n",
    "<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML0101ENSkillsNetwork20718538-2021-01-01\" target=\"_blank\">Joseph Santarcangelo</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--## Change Log\n",
    " -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "prev_pub_hash": "02a6a68144b18d0306d34f05e1db5e78dc2a0798710c3ea8d715eafd28d3a6db"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
